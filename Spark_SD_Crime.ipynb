{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_SD_Crime.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3SnYizmxlCHK5CJt+gDOz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tingyiwu714/san-diego-crime-analysis/blob/master/Spark_SD_Crime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in8la4g941ZM",
        "colab_type": "text"
      },
      "source": [
        "# San Diego Crime Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F_TCl_4cOBl",
        "colab_type": "text"
      },
      "source": [
        "Analysis of crimes in San Diego County from 2007 to 2017 using Apache Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR5bk_E6WGMA",
        "colab_type": "text"
      },
      "source": [
        "## Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dxW-rKfWTYI",
        "colab_type": "text"
      },
      "source": [
        "1. Data Exploration\n",
        "2. Data Visualization\n",
        "3. Conclusion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYHGmFht6_Xz",
        "colab_type": "text"
      },
      "source": [
        "## 0: Setup and Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG33CTQCV-Rb",
        "colab_type": "text"
      },
      "source": [
        "### 0.1 Set up Google Drive environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKF867kxKqVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Spark, Java and findspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "# Initilize pyspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Start spark session\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkNFGmB1sZmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install geocoding library \n",
        "!pip install geopy\n",
        "\n",
        "# Install folium\n",
        "!pip install folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6KQbHRiTKHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from functools import reduce\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import Row\n",
        "\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47xHUb5Ii5sC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Connect Google Colab with Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD_Wv6WrWIWp",
        "colab_type": "text"
      },
      "source": [
        "### 0.2 Load datasets into Spark DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOCtLdttqM4u",
        "colab_type": "text"
      },
      "source": [
        "The data was adapted from the San Diego Regional Data Library. It includes all valid crimes reported to the San Diego County Police Departments from 2007 to 2017. \n",
        "\n",
        "The data is separated in years and in different format.\n",
        "*   Datasets of 2007 to 2011 are in csv format ([link](https://data.sandiegodata.org/dataset/raw-san-diego-county-crime-incidents-2007-2013/))\n",
        "*   Datasets of 2012 to 2017 are in xlsx format  ([link](https://data.sandiegodata.org/dataset/raw-san-diego-county-crime-incidents-2012-2017/))\n",
        "\n",
        "The data will be loaded in Spark DataFrames from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPFWhiAATmgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load crime data between 2007 to 2011\n",
        "# file_id = ['1GynC_phtJr_ycck_FwP6U_1G4-wGjjrb']\n",
        "file_id = ['16GBta4t4zAWO0yr7w7dLDyo7Ji34zzWf']\n",
        "# file_id = ['16GBta4t4zAWO0yr7w7dLDyo7Ji34zzWf',\n",
        "#            '1EblPEoGj4x8hvxLzjsDZh-IKeYhELE18',\n",
        "#            '1tbURDD5QDeAgaHgGvZxIBIrkmQIxFPeh',\n",
        "#            '1WuFB52qSr-dnB5--u5jYwog5Uno12tEK',\n",
        "#            '1QdadT4p1O-FJdjfFlSkSI2Y9dFN-cKdq']\n",
        "mySchema = StructType([StructField(\"activityType\", StringType(), True),\n",
        "                       StructField(\"AGENCY\", StringType(), True),\n",
        "                       StructField(\"activityDate\", StringType(), True),\n",
        "                       StructField(\"LEGEND\", StringType(), True),\n",
        "                       StructField(\"Charge_Description\", StringType(), True),\n",
        "                       StructField(\"BLOCK_ADDRESS\", StringType(), True),\n",
        "                       StructField(\"City_Name\", StringType(), True),\n",
        "                       StructField(\"ZipCode\", StringType(), True)])\n",
        "sdfs = []\n",
        "for id in file_id:\n",
        "  link = 'https://drive.google.com/uc?export=download&id={FILE_ID}'\n",
        "  url = link.format(FILE_ID=id)\n",
        "  pdf = pd.read_csv(url, dtype=str)\n",
        "  spd = spark.createDataFrame(pdf, schema=mySchema)\n",
        "  sdfs.append(spd)\n",
        "df_07to11 = reduce(DataFrame.unionAll, sdfs)"
      ],
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k6RSmjJ__b0l",
        "colab": {}
      },
      "source": [
        "# Load crime data between 2012 to 2017\n",
        "# file_id = ['12VYPF8HeJH1CpDfsM3fbn2S77KBhD6pv']\n",
        "file_id = ['1TpeuhgB7IHa7IDupjkSoY_CdLHQCdIhd']\n",
        "# file_id = ['1TpeuhgB7IHa7IDupjkSoY_CdLHQCdIhd',\n",
        "#            '1WgMUozlgojzrc3RPREimLrcToY1yoRaA',\n",
        "#            '1d05-6gtJYEanI6W-9NhteVEzL6mdpXe_',\n",
        "#            '1VdAikAd1LRkh7Z81bVv61R_A4RUespTf',\n",
        "#            '159AY4OxMvX-XF00FqFQW4KPy1aJOWjMJ',\n",
        "#            '18A3yryRU2q873W149H653jl_UgxWsM_-']\n",
        "mySchema = StructType([StructField(\"reportingYear\", StringType(), True),\n",
        "                       StructField(\"reportingMonth\", StringType(), True),\n",
        "                       StructField(\"agency\", StringType(), True),\n",
        "                       StructField(\"activityStatus\", StringType(), True),\n",
        "                       StructField(\"activitydate\", StringType(), True),\n",
        "                       StructField(\"numberActualReported\", StringType(), True),\n",
        "                       StructField(\"BLOCK_ADDRESS\", StringType(), True),\n",
        "                       StructField(\"city\", StringType(), True),\n",
        "                       StructField(\"zipCode\", StringType(), True),\n",
        "                       StructField(\"censusTract\", StringType(), True),\n",
        "                       StructField(\"censusBlock\", StringType(), True),\n",
        "                       StructField(\"CrimeCategory\", StringType(), True),\n",
        "                       StructField(\"CrimeDescription\", StringType(), True)])\n",
        "sdfs = []\n",
        "for id in file_id:\n",
        "  link = 'https://drive.google.com/uc?export=download&id={FILE_ID}'\n",
        "  url = link.format(FILE_ID=id)\n",
        "  pdf = pd.read_excel(url, dtype=str)\n",
        "  spd = spark.createDataFrame(pdf, schema=mySchema)\n",
        "  sdfs.append(spd)\n",
        "df_12to17 = reduce(DataFrame.unionAll, sdfs)"
      ],
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPyFFMKdQruY",
        "colab_type": "text"
      },
      "source": [
        "## 1: Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NYBPMTxfUaJ",
        "colab_type": "text"
      },
      "source": [
        "These crime incident records are not cleaned, processed or geocoded, and they are inconsistent in many ways. In this part, I will do data cleaning and processing including:\n",
        "\n",
        "\n",
        "*   **Merge two datasets** (2007-2011 and 2012-2017): Two datasets have different column name, and the datasets of 2007-2011 have less columns then 2012-2017. \n",
        "*   **Handle missing values**: Delete rows contain null and NaN\n",
        "*   **Parsing dates**: Convert data to timestamp. There are 3 different time formats in the data\n",
        "*   **Handle inconsistent data**: Crime categories are organized differently between 2007-2011 and 2012-2017 data.\n",
        "*   **Geocoding**: Fix typo of the address and convert to geographic coordinates\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-EBdMTfQ50a",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Understand Raw Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9iEFX8Bo8Y5",
        "colab_type": "text"
      },
      "source": [
        "Total of 2M rows of individual crime incidents. It includes the details of each incident."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xblWZTSLSnZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "0fee898f-5b83-4cc8-886d-11717c78014f"
      },
      "source": [
        "print(\"Number of rows: \", df_07to11.count())\n",
        "print(\"Number of cols: \", len(df_07to11.columns))\n",
        "df_07to11.show(5)"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows:  188669\n",
            "Number of cols:  8\n",
            "+------------+--------------------+--------------------+-------------+--------------------+--------------------+-----------+-------+\n",
            "|activityType|              AGENCY|        activityDate|       LEGEND|  Charge_Description|       BLOCK_ADDRESS|  City_Name|ZipCode|\n",
            "+------------+--------------------+--------------------+-------------+--------------------+--------------------+-----------+-------+\n",
            "|  CRIME CASE| Carlsbad Police, CA|Jan 1, 2007 12:00...|THEFT/LARCENY|GRAND THEFT:MONEY...|7100  BLOCK AVIAR...|   CARLSBAD|  92009|\n",
            "|  CRIME CASE|Chula Vista Polic...|Jan 1, 2007 12:00...|        FRAUD|               FRAUD|300  BLOCK SANDST...|CHULA VISTA|  91911|\n",
            "|  CRIME CASE|Chula Vista Polic...|Jan 1, 2007 12:00...|        FRAUD|               FRAUD|900  BLOCK PAPPAS...|CHULA VISTA|  91911|\n",
            "|  CRIME CASE|Chula Vista Polic...|Jan 1, 2007 12:00...|THEFT/LARCENY|GRAND THEFT:MONEY...|1300  BLOCK MESA ...|CHULA VISTA|  91910|\n",
            "|  CRIME CASE|Escondido Police, CA|Jan 1, 2007 12:00...|        FRAUD|               FRAUD|1500  BLOCK RIMRO...|  ESCONDIDO|  92027|\n",
            "+------------+--------------------+--------------------+-------------+--------------------+--------------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0AkzqeSSpSV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "991d0815-f894-45dc-df0e-d4d1a88fd292"
      },
      "source": [
        "print(\"Number of rows: \", df_12to17.count())\n",
        "print(\"Number of cols: \", len(df_12to17.columns))\n",
        "df_12to17.show(5)"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows:  193552\n",
            "Number of cols:  13\n",
            "+-------------+--------------+--------+---------------+-------------------+--------------------+--------------------+----------+-------+-----------+-----------+---------------+--------------------+\n",
            "|reportingYear|reportingMonth|  agency| activityStatus|       activitydate|numberActualReported|       BLOCK_ADDRESS|      city|zipCode|censusTract|censusBlock|  CrimeCategory|    CrimeDescription|\n",
            "+-------------+--------------+--------+---------------+-------------------+--------------------+--------------------+----------+-------+-----------+-----------+---------------+--------------------+\n",
            "|         2012|             1|CARLSBAD|OPEN - WORKABLE|Aug 26 2011 11:00AM|                   1|0  BLOCK UNKNOWN ...|  CARLSBAD|    NaN|          0|          0|  Part II Crime|               FRAUD|\n",
            "|         2012|             1|CARLSBAD|OPEN - WORKABLE|Dec  1 2011  8:00AM|                   1|3100  BLOCK EL CA...|  CARLSBAD|  92010|      19803|       1024|Larceny >= $400|LARCENY +$400/FRO...|\n",
            "|         2012|             1|CARLSBAD|OPEN - WORKABLE|Dec  1 2011 12:00AM|                   1|600  BLOCK 02ND S...| ENCINITAS|  92024|      17501|       2027|  Part II Crime| OTHER PART II CRIME|\n",
            "|         2012|             1|CARLSBAD|OPEN - WORKABLE|Dec  1 2011 12:00PM|                   1|2100  BLOCK ISLAN...|SAN MARCOS|  92078|      20019|       3003|  Part II Crime|  OTHER NON-CRIMINAL|\n",
            "|         2012|             1|CARLSBAD|OPEN - WORKABLE|Dec  3 2011 12:00PM|                   1|3600  BLOCK GARFI...|  CARLSBAD|  92008|      18000|       2006|Larceny >= $400|LARCENY +$400/FRO...|\n",
            "+-------------+--------------+--------+---------------+-------------------+--------------------+--------------------+----------+-------+-----------+-----------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJHTlvwYRL1S",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Data Cleaning and Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md5XJyK9Y8Wr",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.1 Merge two DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMZRB04i-dYO",
        "colab_type": "text"
      },
      "source": [
        "Here, I will only keep the columns contain useful information including occurred date, description and location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xE5LrfAZB4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rename columns\n",
        "df_07to11 = df_07to11.withColumnRenamed('activityDate', 'date')\\\n",
        "                     .withColumnRenamed('LEGEND', 'category')\\\n",
        "                     .withColumnRenamed('Charge_Description', 'description')\\\n",
        "                     .withColumnRenamed('City_Name', 'city')\n",
        "for col in df_07to11.columns:\n",
        "    df_07to11 = df_07to11.withColumnRenamed(col, col.lower())\n",
        "\n",
        "df_12to17 = df_12to17.withColumnRenamed('activityDate', 'date')\\\n",
        "                     .withColumnRenamed('CrimeCategory', 'category')\\\n",
        "                     .withColumnRenamed('CrimeDescription', 'description')\n",
        "for col in df_12to17.columns:\n",
        "    df_12to17 = df_12to17.withColumnRenamed(col, col.lower())"
      ],
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g8HhcZYJ5RZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Union two dataframes and remove duplicates\n",
        "cols = [\"date\", \"category\", \"description\", \"block_address\", \"city\", \"zipcode\"]\n",
        "df1 = df_07to11.select(cols)\n",
        "df2 = df_12to17.select(cols)\n",
        "df = df1.union(df2).distinct()"
      ],
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqu4v9l5xw6Q",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.2 Missing value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV1J1MPz2TKP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "102b449c-f819-494f-fffd-8349676ad060"
      },
      "source": [
        "# Count missing values of each columns\n",
        "missing = df.select([count(when(isnan(c), c)).alias(c) for c in df.columns])\n",
        "print(\"Number of missing data per column:\")\n",
        "missing.show()"
      ],
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of missing data per column:\n",
            "+----+--------+-----------+-------------+----+-------+\n",
            "|date|category|description|block_address|city|zipcode|\n",
            "+----+--------+-----------+-------------+----+-------+\n",
            "|   0|       0|          0|         1638|3832|  22962|\n",
            "+----+--------+-----------+-------------+----+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcttYDpj_FYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop the rows with all NaN values\n",
        "# Drop the rows with all location columns are NaN\n",
        "df = df.filter(df.date != 'NaN')\\\n",
        "       .filter((df.block_address != 'NaN') & (df.city != 'NaN') & (df.zipcode != 'NaN'))"
      ],
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRug4epDPKUf",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.3 Parsing dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Gh_iqDRbRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove extra space\n",
        "df = df.withColumn('date', regexp_replace('date', '  ', ' '))"
      ],
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngcg3dnDPIzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert \"date\" column to datetime\n",
        "from pyspark.sql.functions import coalesce, col, to_date, to_timestamp\n",
        "def my_to_date(col, frmts=(\"MMM d, y H:m:s a\", \"M/d/y H:m\", \"MMM d y H:ma\")):\n",
        "  return coalesce(*[to_timestamp(col, i) for i in frmts])\n",
        "\n",
        "df = df.withColumn(\"date\", my_to_date(df.date))"
      ],
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgCokG3It2Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract year and month from date\n",
        "df = df.withColumn('year', year(df.date))\\\n",
        "       .withColumn('month', month(df.date))"
      ],
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPoC1KkdPaFR",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.4 Inconsistent data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLABhfBzWxTP",
        "colab_type": "text"
      },
      "source": [
        "Based on Uniform Crime Reporting (UCR), crimes are divided into two major groups: Part I crimes and Part II crimes. Part I crimes are broken into two categories: violent and property crimes. Part II crimes are all other crimes outside of Part I crimes.\n",
        "\n",
        "\n",
        "\n",
        "*   Part I crime\n",
        "    * Violent crime: homicide, rape, robbery, aggravated assault\n",
        "    * Property crime: burglary, larceny-thef, motor vehicle theft, arson\n",
        "\n",
        "*   Part I crime: simple assault, drug, fraud, sex offense, DUI, etc.\n",
        "\n",
        "Here, I'll categorize the crime category follow the UCR guideline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs_U7xoVPZXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "3474651a-817a-4ed0-85d4-2d6154c16abd"
      },
      "source": [
        "diff = df.select(\"category\").distinct()\n",
        "diff.show()"
      ],
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|            category|\n",
            "+--------------------+\n",
            "|               FRAUD|\n",
            "|       Vehicle Theft|\n",
            "|             WEAPONS|\n",
            "|      Simple Assault|\n",
            "|DRUGS/ALCOHOL VIO...|\n",
            "|     Larceny >= $400|\n",
            "|       THEFT/LARCENY|\n",
            "|               ARSON|\n",
            "|                Rape|\n",
            "|               Arson|\n",
            "|          SEX CRIMES|\n",
            "|             ASSAULT|\n",
            "|                 DUI|\n",
            "| MOTOR VEHICLE THEFT|\n",
            "|    Non Res Burglary|\n",
            "|VEHICLE BREAK-IN/...|\n",
            "|       Part II Crime|\n",
            "|        Res Burglary|\n",
            "|             ROBBERY|\n",
            "|            HOMICIDE|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlzBaexwbpFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.withColumn('category', regexp_replace('category', 'FRAUD', 'Part II Crime'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Vehicle Theft', 'Motor Vehicle Theft'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'WEAPONS', 'Part II Crime'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Simple Assault', 'Part II Crime'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'DRUGS/ALCOHOL VIOLATIONS', 'Part II Crime'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Larceny >= $400', 'Larceny-theft'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'THEFT/LARCENY', 'Larceny-theft'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'SEX CRIMES', 'Part II Crime'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'ASSAULT', 'Aggravated Assault'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'DUI', 'Part II Crime'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Non Res Burglary', 'Burglary'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'VEHICLE BREAK-IN/THEFT', 'Part II Crime'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Res Burglary', 'Burglary'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'HOMICIDE', 'Homicide'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Strong ArmRobbery', 'Robbery'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Murder', 'Homicide'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Armed Robbery', 'Robbery'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'VANDALISM', 'Part II Crime'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Larceny < $400', 'Larceny-theft'))\\\n",
        "        .withColumn('category', initcap('category'))\\\n",
        "        .withColumn('category', regexp_replace('category', 'Part Ii Crime', 'Part II Crime'))"
      ],
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydj9H5qTdIXc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "cdbc64a0-780f-4a5f-8988-1ccc1a5792c6"
      },
      "source": [
        "diff = df.select(\"category\").distinct()\n",
        "diff.show()"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|           category|\n",
            "+-------------------+\n",
            "|            Robbery|\n",
            "|    Larceny >= $400|\n",
            "|      Larceny-theft|\n",
            "|               Rape|\n",
            "|              Arson|\n",
            "|           Homicide|\n",
            "|           Burglary|\n",
            "|      Part II Crime|\n",
            "|Motor Vehicle Theft|\n",
            "|     Larceny < $400|\n",
            "| Aggravated Assault|\n",
            "+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukUOWx9KdtV5",
        "colab_type": "text"
      },
      "source": [
        "After processing, crime category only has the 8 serious crimes of Part I crime and all other crimes as Part II crime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLtzoA9qVG9I",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.5 Geocoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn8ShofMR8SM",
        "colab_type": "text"
      },
      "source": [
        "The data only contains physical address of each record. I'll perform geocoding to convert the address into geographic coordinates. Before geocoding, I'll parse the address so that it can be geocoded successfully.\n",
        "\n",
        "Since geocoding take long time, I'll save the processed data to csv files after partial data being geocoded in case of interruption for any reason."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjrJUkhr40GL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine block address, city, and zipcode to get full address\n",
        "# Remove \"BLOCK\", extra whitespaces, and fix typo\n",
        "# Remove \"0\" from \"01ST\", \"02ND\", \"03RD\", \"04TH\", ...\n",
        "df = df.withColumn('full_address', concat(df.block_address, lit(\", \"), df.city, lit(\", CA \"), df.zipcode))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', ' BLOCK ', ' '))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', '  ', ' '))\\\n",
        "       .withColumn('full_address', trim('full_address'))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', ' CAM ', ' CAMINO '))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', ' CAMTO ', ' CAMINITO '))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', ' AVNDA ', ' AVENIDA '))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', ' CVENIDA ', ' AVENIDA '))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', ' TRZA ', ' TERRAZA '))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', ' CR DRIVE', ' CIRCLE'))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', ' MC ', ' MC'))\\\n",
        "       .withColumn('full_address', regexp_replace('full_address', '(\\s)(0)(\\d(ST|ND|RD|TH)\\s)', '$1$3'))"
      ],
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qiIdljNqjs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "df_pd = df.toPandas()\n",
        "df = df_pd.dropna(axis=0, subset=['date'])"
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIltWWR-3Qk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def geocode_my_address(df):\n",
        "  try:\n",
        "    x = geolocator.geocode(df.full_address)\n",
        "    df['longitude'] = x.longitude\n",
        "    df['latitude'] = x.latitude\n",
        "    return df\n",
        "  except:\n",
        "    # print(\"problem with address:\", addr)\n",
        "    df['longitude'] = \"NaN\"\n",
        "    df['latitude'] = \"NaN\"\n",
        "    return df"
      ],
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpgbEbLatwI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Geocode the data by year and month\n",
        "# Save the data to Google Drive after processing a month of data \n",
        "df_geo = pd.DataFrame() \n",
        "geolocator = Nominatim(timeout=10, user_agent = \"dlab.berkeley.edu-workshop\")\n",
        "for yr in range(2007, 2018):\n",
        "  link = '/drive/My Drive/Colab Notebooks/data{}.csv'.format(yr)\n",
        "  df_cur_yr = pd.DataFrame() \n",
        "  for mo in range(1, 13):\n",
        "    df_cur_mo = df_pd.loc[(df_pd['year'] == yr) & (df_pd['month'] == mo)]\n",
        "    df_cur_mo = df_cur_mo.apply(geocode_my_address, axis=1)\n",
        "    df_cur_yr = pd.concat([df_cur_yr, df_cur_mo], ignore_index=True)\n",
        "    df_cur_yr.to_csv(link)\n",
        "  df_geo = pd.concat([df_geo, df_cur_yr], ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqOP1LuBKi3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_geo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j79iI6x5E9fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_geo.to_csv('/drive/My Drive/Colab Notebooks/data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWIcYUT7jHha",
        "colab_type": "text"
      },
      "source": [
        "The data is cleaned, processed, and geocoded. It's ready for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v58XyZOJRQly",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIFYhINFnkGu",
        "colab_type": "text"
      },
      "source": [
        "Geospatial Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzTKq5NKCWHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a map\n",
        "m = folium.Map(location=[32.817316, -117.043098], tiles='cartodbpositron', zoom_start=10)\n",
        "\n",
        "heat_df = df_pd[['latitude', 'longitude']]\n",
        "heat_df = heat_df.dropna(axis=0, subset=['latitude','longitude'])\n",
        "\n",
        "HeatMap(data=heat_df[['latitude', 'longitude']], radius=10).add_to(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0AnHlqhcG9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        m = folium.Map(location=[32.817316, -117.043098], tiles='cartodbpositron', zoom_start=10)\n",
        "        HeatMap(data=df_pd[['latitude', 'longitude']], radius=10).add_to(m)\n",
        "        \n",
        "        df_pd['latitude'] = df_pd['latitude'].astype(float)\n",
        "        df_pd['longitude'] = df_pd['longitude'].astype(float)\n",
        "        \n",
        "        # Filter the DF for rows, then columns, then remove NaNs\n",
        "        heat_df = df_pd[['latitude', 'longitude']]\n",
        "        heat_df = heat_df.dropna(axis=0, subset=['latitude','longitude'])\n",
        "        \n",
        "        # List comprehension to make out list of lists\n",
        "        heat_data = [[row['latitude'],row['longitude']] for index, row in heat_df.iterrows()]\n",
        "        \n",
        "        # Plot it on the map\n",
        "        HeatMap(heat_data).add_to(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n13rW1h1q9uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a map\n",
        "m = folium.Map(location=[32.817316, -117.043098], tiles='cartodbpositron', zoom_start=10)\n",
        "\n",
        "# HeatMap(data=df_pd[['latitude', 'longitude']], radius=10).add_to(m)\n",
        "\n",
        "df_pd['latitude'] = df_pd['latitude'].astype(float)\n",
        "df_pd['longitude'] = df_pd['longitude'].astype(float)\n",
        "\n",
        "# Filter the DF for rows, then columns, then remove NaNs\n",
        "heat_df = df_pd[['latitude', 'longitude']]\n",
        "heat_df = heat_df.dropna(axis=0, subset=['latitude','longitude'])\n",
        "\n",
        "# List comprehension to make out list of lists\n",
        "heat_data = [[row['latitude'],row['longitude']] for index, row in heat_df.iterrows()]\n",
        "\n",
        "# Plot it on the map\n",
        "HeatMap(heat_data).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}